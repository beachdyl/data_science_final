---
title: "Project"
author: "Dylan Bowerman - `dbowerman7652@floridapoly.edu`"
output: html_document
---
# Introduction

In this project, I will be analyzing data from a social experiment where members of an online forum were given the opportunity to increment a running counter by 1 digit. A robot ensured that only the next number in sequence was accepted, so incorrect numbers or illegal/non-numerical entries (essentially all spam) were deleted immediately and do not appear in this data. Users are also not allowed to count multiple numbers in a row (meaning the user that counts the number 108 cannot also count 109). It started at 1 and, as of the time this data was gathered, reached 36597.
Participants were incentivized to participate by being rewarded outside the context of the counting, usually with access to other areas of the forum. Many users were also intrinsically motivated by having access to a running scoreboard where they receive 1 point for each correct number that they count; this created a sense of competition in some people to reach the highest score. In my observations, the participants also self-selected "milestones" that they would try to reach, from small ones such as the next hundred, to large ones such as the next ten thousand. More rare milestones were more desired; the competition for numbers such as 10000, 20000, and 30000 was very harsh, since only the person who sends the number first gets credit for it.
In this document, I explore the trends of how these numbers were counted, fun facts about the numbers and counting patterns, and statistical data of the numbers.

# Libraries
I first load required packages
```{r message=FALSE}
library(readr)
library(tidyr)
library(lubridate)
library(dplyr)
library(ggplot2)
```

# Data Loading and Initial Processing
I import the data in its raw form into a dataset named `counting`. 
```{r message=FALSE, warning=FALSE}
counting <- read_csv("../data/counting_raw.csv")
```

## Base Data Processing
I format the `Date` to an acceptable format so it can be manipulated.
I also add a column called `Delay` which tracks how long (in minutes) it took for that number to be counted, after the prior number.
```{r}
counting <- counting%>%
  mutate(Date = as.POSIXct(strptime(Date,"%Y/%m/%d %H:%M")))%>%
  mutate(Delay=difftime(Date,lag(Date),units="mins"))%>%
  mutate(Attachments=NULL,Reactions=NULL)
head(counting)
```

## Derived Datasets
In this analysis, I am going to analyze the time difference between two numbers that _may_ not be sequential, such as 300 and 400, to determine statistical data for larger ranges of counting. To do this, I break up the main dataset `counting` into 7 smaller datasets containing only numbers divisible by:
* 10
* 50
* 100
* 500
* 1,000
* 5,000
* 10,000.

### The Number 1
In order to calculate the time between the first number ever counted (which is 1) and the first number in the derived datasets, the derived datasets must contain the entry for 1, even though it does not fit their normal rules.
To accomplish this, I start by isolating that row from `counting` in its own table, called `one`. I also remove the `Delay` attribute, because there is no need to calculate that in a dataset with only one value.
```{r}
one <- counting%>%
  filter(Content==1)%>%
  select(-Delay)
one
```

### Deriving the Other Datasets
I then derive the 7 new datasets. Each one is saved as a table containing only the numbers divisible by a certain number, which is also used in word form as the name of the table.
Because I am removing many rows, the `Delay` field becomes inaccurate, so it is removed. Then, I add in the `one` row and arrange by number to bring 1 to the top. I then reestablish the `Delay` field.
```{r message=FALSE}
tens <- counting%>%
  filter(Content%%10==0)%>%
  select(-Delay)%>%
  add_row(one)%>%
  arrange(Content)%>%
  mutate(Delay=difftime(Date,lag(Date),units="mins"))

fifties <- counting%>%
  filter(Content%%50==0)%>%
  select(-Delay)%>%
  add_row(one)%>%
  arrange(Content)%>%
  mutate(Delay=difftime(Date,lag(Date),units="mins"))

hundreds <- counting%>%
  filter(Content%%100==0)%>%
  select(-Delay)%>%
  add_row(one)%>%
  arrange(Content)%>%
  mutate(Delay=difftime(Date,lag(Date),units="mins"))

fivehundreds <- counting%>%
  filter(Content%%500==0)%>%
  select(-Delay)%>%
  add_row(one)%>%
  arrange(Content)%>%
  mutate(Delay=difftime(Date,lag(Date),units="mins"))

thousands <- counting%>%
  filter(Content%%1000==0)%>%
  select(-Delay)%>%
  add_row(one)%>%
  arrange(Content)%>%
  mutate(Delay=difftime(Date,lag(Date),units="mins"))

fivethousands <- counting%>%
  filter(Content%%5000==0)%>%
  select(-Delay)%>%
  add_row(one)%>%
  arrange(Content)%>%
  mutate(Delay=difftime(Date,lag(Date),units="mins"))

tenthousands <- counting%>%
  filter(Content%%10000==0)%>%
  select(-Delay)%>%
  add_row(one)%>%
  arrange(Content)%>%
  mutate(Delay=difftime(Date,lag(Date),units="mins"))
```

# Data Analysis
Now, I must analyze the data. I have a few plans for how to go about this. I'd like to figure out the mean, median, and minimum, and maximum times for each of the datasets (including `counting`, of course). I'd also like to come to some abstract conclusions.

## Timing Matrix
In order to create a matrix of time statistics, I need to create a set of conclusions for each dataset I have. To start with `counting`, I need to calculate the:
* Average (mean) time to count up 1 number
* Median time to count up 1 number
* Fastest (minimum) time to count up 1 number
* Slowest (maximum) time to count up 1 number

### Calculating from Main Dataset
To accomplish this, I create a data frame containing 4 columns: `metric`, `difference`, `value`, and `unit`. This is designed to be read as "The `metric` `difference` number(s) is `value` `units`."
I vary the units for each observation because as the numbers get larger, the data would become difficult to understand if it were always represented in minutes. Sometimes, it is just easier to say "21 days" than "30243 minutes."
I save each data frame as `conclusionsX`, where X is the number that the counting numbers are divided by.
```{r message=FALSE}
conclusions1 <- data.frame(metric=c("Average time to count up","Median time to count up","Fastest time to count up","Slowest time to count up"),
                          difference=c(rep(1,times=4)),
                          value=as.numeric(c(round(sum(counting$Delay,na.rm=T)/nrow(counting),1),round(median(counting$Delay,na.rm=T),1),round(min(counting$Delay,na.rm=T),1),round(max(counting$Delay,na.rm=T)/60/24,1))),
                          unit=c("minutes","minutes","minutes","days"))
```

### Calculating from Derived Datasets
As I do this for the other data frames, I reuse the code and change the `difference` field, as well as replacing the data source in the mathematical formulas to the appropriate table (such as replacing `counting` with `tens` to process with only the numbers divisible by 10). Other than that change, the conclusions stay the same. The `unit` field is changed as I deem it appropriate; when I change a unit, I also update the formula to reflect this new operation.
```{r}
conclusions10 <- data.frame(metric=c("Average time to count up","Median time to count up","Fastest time to count up","Slowest time to count up"),
                          difference=c(rep(10,times=4)),
                          value=as.numeric(c(round((sum(tens$Delay,na.rm=T)/(nrow(tens)-1))/60,1),round(median(tens$Delay,na.rm=T),1),round(min(tens$Delay,na.rm=T),1),round(max(tens$Delay,na.rm=T)/60/24,1))),
                          unit=c("hours","minutes","minutes","days"))

conclusions50 <- data.frame(metric=c("Average time to count up","Median time to count up","Fastest time to count up","Slowest time to count up"),
                          difference=c(rep(50,times=4)),
                          value=as.numeric(c(round((sum(fifties$Delay,na.rm=T)/(nrow(fifties)-1))/60,1),round((median(fifties$Delay,na.rm=T))/60,1),round(min(fifties$Delay,na.rm=T),1),round(max(fifties$Delay,na.rm=T)/60/24,1))),
                          unit=c("hours","hours","minutes","days"))

conclusions100 <- data.frame(metric=c("Average time to count up","Median time to count up","Fastest time to count up","Slowest time to count up"),
                          difference=c(rep(100,times=4)),
                          value=as.numeric(c(round((sum(hundreds$Delay,na.rm=T)/(nrow(hundreds)-1))/60,1),round((median(hundreds$Delay,na.rm=T))/60,1),round(min(hundreds$Delay,na.rm=T),1),round(max(hundreds$Delay,na.rm=T)/60/24,1))),
                          unit=c("hours","hours","minutes","days"))

conclusions500 <- data.frame(metric=c("Average time to count up","Median time to count up","Fastest time to count up","Slowest time to count up"),
                          difference=c(rep(500,times=4)),
                          value=as.numeric(c(round((sum(fivehundreds$Delay,na.rm=T)/(nrow(fivehundreds)-1))/60/24,1),round((median(fivehundreds$Delay,na.rm=T))/60/24,1),round(min(fivehundreds$Delay,na.rm=T),1),round(max(fivehundreds$Delay,na.rm=T)/60/24/30,1))),
                          unit=c("days","days","minutes","months"))

conclusions1000 <- data.frame(metric=c("Average time to count up","Median time to count up","Fastest time to count up","Slowest time to count up"),
                          difference=c(rep(1000,times=4)),
                          value=as.numeric(c(round((sum(thousands$Delay,na.rm=T)/(nrow(thousands)-1))/60/24,1),round((median(thousands$Delay,na.rm=T))/60/24,1),round((min(thousands$Delay,na.rm=T))/60,1),round(max(thousands$Delay,na.rm=T)/60/24/30,1))),
                          unit=c("days","days","hours","months"))

conclusions5000 <- data.frame(metric=c("Average time to count up","Median time to count up","Fastest time to count up","Slowest time to count up"),
                          difference=c(rep(5000,times=4)),
                          value=as.numeric(c(round((sum(fivethousands$Delay,na.rm=T)/(nrow(fivethousands)-1))/60/24/30,1),round((median(fivethousands$Delay,na.rm=T))/60/24/30,1),round((min(fivethousands$Delay,na.rm=T))/60/24,1),round(max(fivethousands$Delay,na.rm=T)/60/24/30,1))),
                          unit=c("months","months","days","months"))

conclusions10000 <- data.frame(metric=c("Average time to count up","Median time to count up","Fastest time to count up","Slowest time to count up"),
                          difference=c(rep(10000,times=4)),
                          value=as.numeric(c(round((sum(tenthousands$Delay,na.rm=T)/(nrow(tenthousands)-1))/60/24/30,1),round((median(tenthousands$Delay,na.rm=T))/60/24/30,1),round((min(tenthousands$Delay,na.rm=T))/60/24,1),round(max(tenthousands$Delay,na.rm=T)/60/24/30,1))),
                          unit=c("months","months","days","months"))
```

### Combining Conclusion Data Frames
Once I have all 8 data frames, I combine them into one data frame just called `conclusions`.
```{r}
conclusions <- rbind(conclusions1,conclusions10,conclusions50,conclusions100,conclusions500,conclusions1000,conclusions5000,conclusions10000)
```

### Generating the Matrix
To make a matrix of it, I isolate the `value` and `unit` variables, since the other two variables are patterned and predictable. I create a matrix with those and name the rows and columns according to the orders I created the data conclusions.
```{r}
matrix <- matrix(paste(conclusions$value,conclusions$unit,sep=" "),nrow=4)
colnames(matrix) <- c(unique(conclusions$difference))
rownames(matrix) <- c(unique(conclusions$metric))
```
From there, a completed matrix exists that shows the metric being measured on each row, the number used as the divisor on the columns, and the corresponding data in the center.
```{r echo=FALSE, message=TRUE}
matrix
```
From here, you can see some interesting conclusions. For one, the fastest time to count up 10 numbers is 0 minutes, which tells us that at least once, users counted 10 numbers before the minute changed. Also, we can see that the fastest time to count up 1 number is -84 minutes. That sounds like an anomaly!

#### Ooh, an Anomaly!
To figure this out, I view the 3 smallest values for `Delay` in `counting`. This shows me that there is only one value below 0, which is good.
```{r}
counting%>%
  arrange(Delay)%>%
  head(3)
```
From here, we can view the numbers around that number (which is 20726).
```{r}
counting%>%
  filter(Content==20724|Content==20725|Content==20726|Content==20727|Content==20728)
```
Here we can see that it was counted apparently 24 minutes _prior to_ the number before it. A quick Google search reveals that November 1st, 2020, the timestamp associated with this discrepancy, was, in fact, the day daylight saving time ended. My guess for where it got -84 is that it took the -24 minute difference between the two messages and added 60 more because of the lost hour of daylight saving time.

#### "But wait, there's more!"
In looking at that snapshot of the data we can also see that the number following our problem number (20727) is also a bit strange! It appears to have occurred 29 minutes after the number preceding it. 01:01 to 01:40. However, yet again it shows up as 99 minutes of `Delay`. Noting that 99 = 29 + 60, I once again conclude that this was an error of processing caused by the mysteries that daylight saving time introduces to the dataset.

#### Just in Case...
Just in case this happened again, I'd like to check to see if this type of error occurred on the next day where daylight saving time became relevant, which is March 14, 2021.
```{r}
counting%>%
  filter(Content==32074|Content==32075|Content==32076)
```
Looking at the numbers counted at that time, it seems this is not an issue, likely just because nobody was attempting to count between 1 and 2 AM. Great news!

## Abstract Conclusions
I also want to reach a set of more abstract conclusions exploring relationships between timing, date, season, and other factors. I am going to calculate these and save them as named variables, then combine them into a data frame later.

### Calculating These
Let's do some math!

#### Numbers Counted Quickly
First, I want to calculate how many numbers are counted very quickly in comparison to the number before. Specifically, what precent of numbers are counted within the same minute as the last number? To do this, I filter for numbers with a `Delay` of 0, since that means they had the same timestamp in `Date`. Then, I count the number of rows that remain and save this in `nfast`, which translates roughly to "__n__umbers counted __fast__ (or quickly)."
Later, I will divide that number by the total number of rows in the `counting` dataset. I will then multiply the result by 100 to convert it to a percent.
```{r}
nfast <- counting%>%
  filter(Delay==0)%>%
  nrow()
```

#### Percent of Numbers Counted During Off Hour
Second, I'd like to calculate the percent of numbers that are counted in the single hour in which the fewest number of numbers are sent. To do this, I establish an `Hour` column from the `Date` timestamp, and group by it. I then summarize by counting the numbers that are present in each hour, which gives me this:
```{r}
counting%>%
  mutate(Hour=hour(Date))%>%
  group_by(Hour)%>%
  summarize(Counts=n())
```
From there, I filter to only include the hour with the fewest numbers, which is 6. Following, I mutate that into a new column where I divide that number by the total number of rows in `counting` and multiply the result by 100 to get a percent. This is saved in `pcatminhr`, which translates roughly to "__p__ercent of numbers __c__ounted __at__ the __min__imum __h__ou__r__."
```{r}
pcatminhr <- counting%>%
  mutate(Hour=hour(Date))%>%
  group_by(Hour)%>%
  summarize(Counts=n())%>%
  filter(Counts==min(Counts))%>%
  mutate(Percent=(Counts/nrow(counting))*100)
```

#### Percent of Numbers Counted During Peak Hour
Third, I'd like to calculate the percent of numbers that are counted in the single hour in which the _greatest_ number of numbers are sent. To do this, I follow the same process as before, essentially only changing the 'min' to 'max.' The peak hour here is 22. I save the resulting data in `pcatmaxhr`, which translates roughly to "__p__ercent of numbers __c__ounted __at__ the __max__imum __h__ou__r__."
```{r}
pcatmaxhr <- counting%>%
  mutate(Hour=hour(Date))%>%
  group_by(Hour)%>%
  summarize(Counts=n())%>%
  filter(Counts==max(Counts))%>%
  mutate(Percent=(Counts/nrow(counting))*100)
```

#### Numbers Skipped
Fourth, I'd like to see how many numbers do not appear in this data, but should. This could be caused by an unknown entity removing them from the forum itself, or an issue with data collection. It could also just be the moderator robot not catching a missed number (which is unlikely, given how robots are). I do this later in the process by subtracting the number of rows in `counting` from the largest number in `counting`.

#### Longest 1 Minute Streak
Fifth, I'd like to see how many numbers the users were able to fit into a single minute timespan at maximum. To do this, I group the data in `counting` by `Date` and summarize that by counting the number of rows associated with each timestamp. From there, it is simple to filter it to the largest value of `Counts`. The results are saved in `maxc1min`, which translates roughly to "__max__imum __n__umbers __c__ounted in __1__ __min__ute."
```{r message=TRUE}
maxnc1min <- counting%>%
  group_by(Date)%>%
  summarize(Counts=n())%>%
  filter(Counts==max(Counts))
```
```{r echo=FALSE}
counting%>%
  group_by(Date)%>%
  summarize(Counts=n())%>%
  filter(Counts==max(Counts))
```

#### Percent of Numbers Counted at Night
Just to see how much of night owls the users of this forum are, I chose to see how many numbers are counted between 6 AM and 6 PM. To do this, I mutated a new column called `Hour` from the `Date` timestamp, then I filtered it to my desired range of before 6 AM or after 6 PM. I then count the number of rows remaining and save this in `pcatnight`, which translates roughtly to "__p__ercent of numbers __c__ounted __at__ __night__."
Later, I will divide this by the number of rows in `counting` and multiply the result by 100 to get a percent.
```{r}
pcatnight <- counting%>%
  mutate(Hour=hour(Date))%>%
  filter(Hour<6|Hour>=18)%>%
  nrow()
```

### Summarizing These
Now that I have the data calculated and a plan for how to process it, I generate a vector for each conclusion and save it under the name `abstractX`, where X is a number increasing by 1 from 1 just to keep them in their (decently arbitrary) order. I make these vectors include the description of the number, and then the numerical value, applying the necesary calculations as discussed above and rounding to 2 decimal places where required. Then, I combine them into a data frame called `abstract`.
```{r}
abstract1 <- c("Percent of numbers counted within the same minute as the previous number:",round((nfast/nrow(counting))*100,2))
abstract2 <- c("Percent of numbers counted during the least popular hour:",round(pcatminhr$Percent,2))
abstract3 <- c("Percent of numbers counted during the most popular hour:",round(pcatmaxhr$Percent,2))
abstract4 <- c("Number of numbers mysteriously skipped during counting (or otherwise not present in data):",max(counting$Content,na.rm=T)-nrow(counting))
abstract5 <- c("Longest streak of numbers counted within the same minute:",maxnc1min$Counts)
abstract6 <- c("Percent of numbers counted at night (6pm-6am):",round(pcatnight/nrow(counting)*100,2))
abstract <- data.frame(description=c(abstract1[1],abstract2[1],abstract3[1],abstract4[1],abstract5[1],abstract6[1]),value=as.numeric(c(abstract1[2],abstract2[2],abstract3[2],abstract4[2],abstract5[2],abstract6[2])))
abstract
```

## Data Visualizations
A useful way to analyze any dataset is with visualization, so I generated a few of those to help convey some information that would otherwise be clunky to convey.

### Numbers Counted by Hour of Day
One of the things I was interested in seeing is how the rate at which numbers are sent varies by time of day. Are more numbers counted at 8 PM than 3 AM? Probably. But I'd like to see it. Do do this, I generate a `Hour` variable, as I have many times before, and group and summarize from that. Then, I plot a bar graph with the `Hour` on the X axis and the number of `Counts` on the Y axis. I also make it Poly Purple (for fun)!
```{r, echo=FALSE}
counting%>%
  mutate(Hour=hour(Date))%>%
  group_by(Hour)%>%
  summarize(Counts=n())%>%
  ggplot(aes(x=Hour,y=Counts))+ggtitle("Numbers Counted by Hour of Day")+geom_col(fill="#532d8e")
```
From this, we can see that counting slows down very much at night, which makes sense, and it also serves to verify our information we concluded earlier regarding the most and least popular hours for counting. Counting is also very popular in the late evening, with the peak being at 10 PM! This makes sense to me, because that is when I am awake, on the internet, and bored.

### Numbers Counted by Week of Year
Another statistic I was interested in calculating is the numbers counted over the whole year. Notably, this is almost exactly 1 year worth of data. I used week numbers to track this, so the first step is to mutate a column with that information. I then group and summarize that. I create a bar graph with `Week` on the X axis and `Counts` on the Y axis. Poly Purple again, too!
```{r}
counting%>%
  mutate(Week=week(Date))%>%
  group_by(Week)%>%
  summarize(Counts=n())%>%
  ggplot(aes(x=Week,y=Counts))+ggtitle("Numbers Counted by Week of Year")+geom_col(fill="#532d8e")
```
It seems counting was very slow in the summer, with just a few numbers being counted each week in weeks 31-33. It then picked up and spiked big time in weeks 41-44, which should be around October. Maybe this is because people were settled into their new school schedules (where applicable)? Possibly.
Looking at the graph, it would appear that counting increased dramatically between weeks 17 and 18 as well. After requesting that data numerically, it seems to confirm that.
```{r}
counting%>%
  mutate(Week=week(Date))%>%
  group_by(Week)%>%
  summarize(Counts=n())%>%
  filter(Week==17|Week==18)
```
That is a nearly 10x increase in counting between the two weeks! However, this is a trap. There is no relationship between the two weeks, because this data _begins_ in Week 18 of 2020 (which has 53 weeks, by the way) and _ends_ at Week 17 of 2021. This makes it convenient to view an entire year, but does hide the fact that week 17 does not lead at all into week 18. In fact, they are as far apart as they possibly can be.
This also could explain week 18 being so tall. At that point in time, the counting experiment was brand new. People may have been excited to try it out, or been rushing to complete the milestones I discussed earlier. Incentives and marketing were likely high to start the project, possibly fueling a rapid start.

### Distribution of Numbers Counted by Top 10 Users
Remembering that the users had access to a scoreboard that incremented when they counted a number, some users were undoubtedly inclined to do _lots_ of counting with the motivation of taking the top spots on that scoreboard. I would like to see how the numbers are distributed among the users in the top 10 spots of the leaderboard. First, I must group and summarize the data in ` counting` by the `Author` field, which is the user that sent the message. I then mutate a new variable called `Percent` which is just an individual user's numbers counted divided by the total of rows in `counting` (times 100 to make a percent). I then arrange that in descending order and chop off all but the top 10 users. Let's look at that data.
```{r}
counting%>%
  group_by(Author)%>%
  summarize(Counts=n())%>%
  mutate(Percent=(Counts/nrow(counting))*100)%>%
  arrange(desc(Percent))%>%
  head(10)
```
This shows the top 10 users and the number of numbers they counted in this experiment, as of when this data was collected. The top user has counted 18% of all 36549 numbers in the dataset! _Impressive!_ Others are clearly not far behind. Let's make that into a pie chart.
```{r}
counting%>%
  group_by(Author)%>%
  summarize(Counts=n())%>%
  mutate(Percent=(Counts/nrow(counting))*100)%>%
  arrange(desc(Percent))%>%
  head(10)%>%
  ggplot(aes(x="",y=Percent,fill=Author))+ggtitle("Distribution of Numbers Counted by Top 10 Users")+geom_bar(stat="identity",width=1,color="white",show.legend=T)+coord_polar("y",start=0)+theme_void()
```
Pie chart appeared! It shows market share (if you will) of the different users in the top 10! From this we see a reasonable distribution between users, but are also able to tell who has the most and least counts among the group. I think this visual is very effective for this reason.

# Conclusions
From this dataset, which is surprisingly malleable, we can learn a lot about this experiment, and maybe forum users at large. From simple things like "'beachdyl#0609' has too much time on their hands" to "they aren't particularly fond of counting over the summer" all the way to complex conclusions like "counting happens least at 6 AM, making up less than a quarter of a percent of the numbers counted." There is definitely more data that can be gathered from this, such as the average counting statistics for individual users, or even ranking users not by total numbers counted, but by the rate at which they counted numbers when they are most active. This could be an interesting way to find out who is the most "ferocious" counter or the most "ghostly" counter (perhaps they count one number and then always wait several thousand numbers before counting again). In conclusion, while there is much more that can be learned from this data, the conclusions and visualizations generated here give a very interesting insight into this social experiment.
